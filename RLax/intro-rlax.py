import collections
import optax
import rlax
import haiku as hk
from haiku import nets
from bsuite.environments import catch
import jax
import jax.numpy as jnp

Transition = collections.namedtuple(
    "Transition", 
    "obs_tm1 a_tm1 r_t discount_t obs_t"
)
ActorOutput = collections.namedtuple("ActorOutput", "actions q_values")

def build_network(num_hidden_unts: int, num_actions: int) -> hk.Transformed:
    """Build a simple MLP for Q-approximations. 

    Args:
        num_hidden_unts (int): number of hidden units 
        num_actions (int): number of actions

    Returns:
        hk.Transformed: Q-Function
    """
    def q(obs):
        flatten = lambda x: jnp.reshape(x, (-1,))
        network = hk.Sequential(
            [flatten, nets.MLP([num_hidden_unts, num_actions])]
        )

        return network(obs)
    return hk.without_apply_rng(hk.transform(q))

class TransitionAccumulator:
    """Simple python accumulator for transitions
    """

    def __init__(self) -> None:
        self.prev = None
        self._action = None
        self._latest = None
    
    def push(self, env_output, action):
        self._prev = self._latest
        self._action = action
        self._latest = env_output

    def sample(self, batch_size):
        """Sample from the action-reward space

        Args:
            batch_size (int): batch size of sample

        Returns:
            Transition: Next sampled state (online learning)
        """
        assert batch_size == 1
        return Transition(self._prev.observation, self._action, self._latest.reward, 
            self._latest.discount, self._lates.observation)
    
    def is_ready(self, batch_size):
        assert batch_size == 1
        return self._prev is not None

class OnlineQ:
    """Online Q-Learning Deep RL Agent
    """

    def __init__(self, observation_spec, action_spec, num_hidden_units, epsilon,
                learning_rate):
        self._observation_spec = observation_spec
        self._action_spec = action_spec
        self._epsilon = epsilon
        
        # initialize the neural network
        self._network = build_network(num_hidden_units, num_actions=action_spec.num_values)
        self._optimizer = optax.adam(learning_rate)

        # jitting for speed
        self.actor_step = jax.jit(self.actor_step)
        self.learner_step = jax.jit(self.learner_step)
    
    def initial_params(self, key):
        """Initalize network parameters

        Args:
            key (): rng key generated by jax

        Returns:
            hk.Transformed: initialized network
        """
        sample_input = self._observation_spec.generate_value()
        return self._network.init(key, sample_input)
    
    def initial_actor_state(self):
        return ()
    
    def initial_learner_state(self, params):
        """Return initialize optimizer

        Args:
            params (dict): Network parameters

        Returns:
            optax.adam: optimizer that initialized parameters
        """
        return self._optimizer.init(params)

    def actor_step(self, params, env_output, actor_state, key, evaluation):
        q = self._network.apply(params, env_output.observation)
        train_a = rlax.epsilon_greedy(self._epsilon).sample(key, q)
        eval_a  = rlax.greedy().sample(key, q)
        a       = jax.lax.select(evaluation, eval_a, train_a)

        return ActorOutput(actions=a, q_values=q), actor_state
    
    def learner_step(self, params, data, learner_state, unused_key):
        dloss_dtheta = jax.grad(self._loss)(params, *data)
        updates, learner_state = self._optimizer.update(dloss_dtheta, learner_state)
        params = optax.apply_updates(params, updates)

        return params, learner_state

    def _loss(self, params, obs_tm1, a_tm1, r_t, discount_t, obs_t):
        q_tm1 = self._network.apply(params, obs_tm1)
        q_t = self._network.apply(params, obs_t)
        td_error = rlax.q_learning(q_tm1, a_tm1, r_t, discount_t, q_t)
        return rlax.l2_loss(td_error)

def main(unused_arg):
    env = catch.Catch(seed=0)
    agent = OnlineQ(
        observation_spec=env.observation_spec(),
        action_spec=env.action_spec(),
        num_hidden_units=100,
        epsilon=1e-3,
        learning_rate=1e-1
    )

    accumulator = TransitionAccumulator()